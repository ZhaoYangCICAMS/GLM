
R version 3.0.2 (2013-09-25) -- "Frisbee Sailing"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # Generalized Linear Model Experiments
> #
> # Compare a logistic regression computed using the usual maximum likelihood
> # estimate with an alternative model computed directly by a nonlinear least
> # squares estimate.
> 
> # We use the 'Contraception' data set from the mlmRev package in favor of a
> # made-up example. You might need to install the mlmRev package to obtain
> # these data. We mostly follow the very concise introduction to generalized
> # linear models by Doug Bates available here:
> # http://www.stat.wisc.edu/courses/st849-bates/lectures/GLMH.pdf
> data("Contraception",package="mlmRev")
> nobs = nrow(Contraception)
> 
> # Compute the usual maximum likeliehood estimate. We use a formula suggested
> # by Doug Bates in the note cited above, worth reading! Note that we also
> # specify x=TRUE, which builds and returns a model matrix for us. We'll use
> # that later.
> MLE = glm(formula = use ~ age + I(age^2) + urban + livch,
+           family = binomial, x=TRUE, data=Contraception)
> 
> # Here is the standard model summary provided by GLM:
> print(summary(MLE))

Call:
glm(formula = use ~ age + I(age^2) + urban + livch, family = binomial, 
    data = Contraception, x = TRUE)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4738  -1.0369  -0.6683   1.2401   1.9765  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.9499521  0.1560118  -6.089 1.14e-09 ***
age          0.0045837  0.0089084   0.515    0.607    
I(age^2)    -0.0042865  0.0007002  -6.122 9.23e-10 ***
urbanY       0.7680975  0.1061916   7.233 4.72e-13 ***
livch1       0.7831128  0.1569096   4.991 6.01e-07 ***
livch2       0.8549040  0.1783573   4.793 1.64e-06 ***
livch3+      0.8060251  0.1784817   4.516 6.30e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2590.9  on 1933  degrees of freedom
Residual deviance: 2417.7  on 1927  degrees of freedom
AIC: 2431.7

Number of Fisher Scoring iterations: 4

> 
> # Let's compare the maximum likelihood estimate with a generic nonlinear
> # least squares solution...
> # Extract the response y and model matrix x from the MLE model object.
> x = MLE$x
> y = MLE$y
> logistic = function(x) 1/(1 + exp(-x))
> f = function(beta)
+ {
+    w  = x %*% beta
+    lw = logistic(w)
+    crossprod(y - lw)[1]
+ }
> NNLS = optim(rep(0,ncol(x)), fn=f, method="BFGS")
> names(NNLS$par) = colnames(x)
> 
> # We can compare the coefficients estimated by each approach. They're
> # somewhat different.
> print(data.frame(MLE=coef(MLE), NNLS=NNLS$par))
                     MLE         NNLS
(Intercept) -0.949952124 -0.905744758
age          0.004583726  0.007883280
I(age^2)    -0.004286455 -0.004448782
urbanY       0.768097459  0.756306526
livch1       0.783112821  0.763205239
livch2       0.854904050  0.825079534
livch3+      0.806025052  0.753420488
> 
> 
> 
> 
> # Let's compare bootstrapped estimates to get better a sense of how these two
> # estimation approaches vary...First, let's obtain bootstrapped estimates of
> # the standard MLE solution:
> set.seed(1)
> mleboot = replicate(1000,
+ {
+   i = sample(nobs,nobs,replace=TRUE)
+   glm.fit(x=x[i,],y=y[i], family=binomial())$coef
+ })
> 
> # And now with the nonlinear least squares solution:
> fboot = function(i)
+ {
+   function(beta)
+   {
+     w  = x[i,] %*% beta
+     lw = logistic(w)
+     crossprod(y[i] - lw)[1]
+   }
+ }
> nnlsboot = replicate(1000,
+ {
+   i  = sample(nobs,nobs,replace=TRUE)
+   f = fboot(i)
+   optim(rep(0,ncol(x)), fn=f, method="BFGS")$par
+ })
> 
> # Let's compare histograms of one of the more significant model variables in
> # this problem, the square of age:
> jpeg(file="compare.jpg",quality=100,width=800,height=800)
> split.screen(c(2,1))
[1] 1 2
> xrange = c(min(min(mleboot[3,]),min(nnlsboot[3,])),
+            max(max(mleboot[3,]),max(nnlsboot[3,])))
> screen(1)
> hist(mleboot[3,],main="age^2 maximum likelihood estimate",col=4,xlim=xrange,breaks=25)
> screen(2)
> hist(nnlsboot[3,],main="age^2 direct nonlinear optimization",col=4,xlim=xrange,breaks=25)
> dev.off()
null device 
          1 
> 
