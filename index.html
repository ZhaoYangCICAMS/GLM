<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>GLMs, abridged</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
<section>
<h1>
<a name="generalized-linear-models-abridged" class="anchor" href="#generalized-linear-models-abridged"><span class="octicon octicon-link"></span></a>Generalized linear models, abridged.
</h1>
<p class="view"><a href="https://github.com/bwlewis/GLM">View the Project on GitHub &nbsp;&nbsp;  bwlewis/GLM</a></p>
<a href="mailto:blewis@illposed.net">Bryan W. Lewis</a> and <a href="mailto:michael.kane@yale.edu">Mike Kane</a>
</section>
<section>

<h2>Introduction</h2>

Generalized linear models (GLMs) are indespensible tools in the data science
toolbox. They are applicable to many real-world problems involving continuous,
yes/no, count and survival data. The models themselves are intuitive and can be
used for inference and prediction.  A few very high quality free and open
source sofware implementations are available (in particular in R[6]), as are
several very good commercial ones (Revolution Analytics[7], SAS, Stata).

<p>
<br/>
Despite the wide applicability of GLMs and the availability of high-quality
reference implementations, we've found it hard to find good high-performance
free and open souce software implementations geard to solving large problems.
Moreover, we've had trouble finding succinct references that deal with the core
GLM ideas and implementation details that could help us build our own
high-performance implementations.

<p>

Our goal for this note is to present the core ideas behind GLMs correctly, but
also as simply as we can. We present basic implementations, some of which solve
GLMs in just a few lines of code.  We discuss implementation details that
enable the development of effective distributed parallel implementations
suitable for solution of large-scale problems.

<p>

We devlop our ideas in our favorite language, R, but they are easily adapted to
other languages.  Python and Boost/C++ are particularly well-equipped for good
GLM implementations in our opinion.

<h2>Disclaimer</h2>

This is <i>not</i> a formal introduction to GLMs.  We focus on illuminating
just a few key ideas, mostly from a numerical point of view and often focusing
on performance and scalability. We refer the reader to the references [3,5,8]
for more complete introductions to GLMs.

<h2>Linear models</h2>

We begin by defining what we mean when we say "linear model" before moving on
to GLMs. Our presentation and notation generally follow Bj&ouml;rk[1],
including the following definition of expected value and variance. Let $y$ be
a random variable with distribution function $F(y)$. The expected value $\mu$ and
the variance $\sigma^2$ of $y$ are defined:
\[
E(y) = \mu = \int_{-\infty}^{\infty} ydF(y),
\qquad \sigma^2 = \int_{-\infty}^{\infty}(y-\mu)^2dF(y).
\]
When $y$ is a vector of random variables, we write $E(y)$ to mean the expected
value applied pointwise to each entry, and similarly we define the expected
value of a matrix of random variables to be the expected value of each entry.
Define the variance-covariance matrix of a vector of random variables $y$ with
vector of expected values $\mu$ to be
\[
V(y) = E(yy^T) - \mu \mu^T.
\]

Let $b\in\mathbb{R}^m, b\ne 0$ be a vector of measurements (called a response
vector) and let $A\in\mathbb{R}^{m\times n}$, $\mathrm{rank}(A)=n$, be a matrix
formed from data observations (called the model matrix).  Then
\[
Ax = b + \epsilon
\]
is what we mean when we say "linear model," where $x\in\mathbb{R}^n$ is a
vector of model coefficients and $E(\epsilon)=0$ and $V(\epsilon)=\sigma^2 I$.
That is, the residual error term $\epsilon$ consists of uncorrelated random
variables with zero mean and the same variance. Given a vector $x$ of model
coefficients, we call the product $Ax$ a predictor of the response
vector $b$.

<p><br/>

Given a model matrix $A$ and response vector $b$, there are many ways we might
go about solving for a vector of model coefficients $x$. We describe a
particularly natural one next.


<h2>Least squares</h2>

One idea we
might think of for solving for the model coefficients would be to compute
the ordinary least squares solution $x$ that minimizes
\[
\min_x ||Ax - b||^2,
\]
where $||\cdot||$ indicates Euclidean norm. Note that the conditions that
$\mathrm{rank}(A)=n$ and $b\ne 0$ imply that there exists a unique least
squares solution.  It turns out that the least squares
solution of linear models has important statistical properties shown in the
seminal work of Gauss[2] and later rediscovered by Markoff[4]. (The least
squares solution defines a <i>minimum variance unbiased estimator</i>, the
technical details of which we leave to the references.)

<h2>Generalized linear models</h2>

Generalized linear models
arise when we constrain the values that the predictor $Ax$ can take on. Such
constraints are important for many practical problems.  For example:
<ul>
<li> The response $b$ may represent binary 0/1 data and we desire the predictor $Ax$
to at least be constrained in the interval $(0,1)$.
<li> $b$ might represent counts and we desire the predictor to be nonnegative since
negative counts might not make any sense.
</ul>
A standard approach to solving the constrained problem is to transform it into
an unconstrained one by way of a <i>link function</i>. A link function is a
bijection chosen to map the constraint interval onto the real line (the
inverse link function maps the real line onto the constraint interval).
For example, the logistic function
\[
g(a) = \frac{1}{1 + \exp(-a)}
\]
transforms arbitrary values in $a\in\mathbb{R}$ onto the interval $(0,1)$.

<p><br/>
Consider the binary 0/1 data example cited above.  Let
$A\in\mathbb{R}^{m\times n}$ be a given model matrix with $\mathrm{rank}(A)=n$, and let
$b\in\mathbb{R}^m$, $b\ne 0$, be a given response vector. The generalized linear model
problem is to compute an $x$ that satisfies:
\[
Ax = b + \epsilon,\qquad \mathrm{such\, that}\,\, Ax\in(0,1).
\]
We can formulate a related unconstrained problem
that solves for $x$ in
\[
g(Ax) = b + \epsilon,
\]
where $g$ is the logistic function applied elementwise to $Ax$. The problem
is no longer constrained because $g$ maps any value into the interval $(0,1)$.
But it's no longer linear either! How might we solve such a problem? Similarly
to the basic linear model presented earlier, one natural idea is to use least
squares.

<h2>Nonlinear least squares</h2>

Let's quickly recap ideas:
<ul>
<li> The generalized linear model problem is similar to an ordinary linear model problem, but
with constraints on the predictor $Ax$.
<li> We can use a link function to transform a GLM into an unconstrained problem, but...
<li> the resulting unconstrained problem is nonlinear.
</ul>
We can try to use nonlinear least squares to solve the transformed GLM problem
just like we used ordinary least squares to sove
the linear model problem. Consider the binary 0/1 data example in the last section, where
the function $g$ is the logistic function.
Let's cook up a nonlinear least sqaures method to solve
\[
\min_x ||g(Ax) - b||^2.
\]
We warn you right now that the nonlinear least squares solution turns
out to <i>not</i> be the most efficient way to solve GLMs. Feel free to skip on
down to the maximum likelihood methods below. But this technique is just so
obvious, and we have found GLMs so rarely discussed in this way elsewhere, that
we feel compelled to explore this solution method.

<p>
The following R function solves the GLM problem by direct application of a
nonlinear least squares solver to the link function-transformed problem. We use
R's <tt>optim</tt> optimization function using Broyden's method (a quasi-Newton
solver&mdash;most nonlinear solvers are variations on Newton or secant
methods).

For example, when <tt>g=binomial()$linkinv</tt> (the logistic function), this
roughly corresponds to logistic regression using R's <tt>glm</tt> function.
Later we will compare the two solutions in a systematic way.

<pre>
# Input: A is an m by n model matrix, b a response vector of length m,
#        g is the link function (for example g=binomial()$linkinv  for logistic).
# Output: The model coefficients x.

nnls_glm <- function(A,b,g)
{
  f <- function(x) crossprod(b - g(A %*% x))[]
  optim(rep(0,ncol(A)), fn=f, method="BFGS")$par
}
</pre>
It's an amazingly short function!


</section>

<section>
<h2>References</h2>

<ol>
<li> Bj&ouml;rck, &Aring;.,  Numerical Methods for Least Squares Problems, SIAM, Philadelphia, 1996.
<li> Gauss, C. F., Theoria combinationis observationum erroribus minimis obnoxiae, Pars prior, 1863 (1st written 1821).
<li> Hastie, T. J. and Pregibon, D., Generalized linear models,
     Chapter 6 of Statistical Models in S, eds J. M. Chambers and T.
     J. Hastie, Wadsworth &amp; Brooks/Cole, 1992.
<li> Markoff, A., Wahrscheinlichheitsrechnug,  Leipzig, 1912.
<li> McCullagh P. and Nelder, J. A., Generalized Linear Models, Chapman and Hall, London 1989.
<li> The R project <a href="http://www.r-project.org">http://www.r-project.org</a>.
<li> Revolution Analytics <a href="http://revolutionanalytics.com">http://revolutionanalytics.com</a>.
<li> Zelen, M., Linear estimation and relted topics, in Survey of Numerical Analysis, Todd. J. ed, McGraw-Hill, New York, 1962, pp. 429-441.
</ol>
</secion>
</div>

<script src="javascripts/scale.fix.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</body>
</html>
