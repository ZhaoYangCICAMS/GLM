<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>GLMs, abridged</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
<section>
<h1>
<a name="generalized-linear-models-abridged" class="anchor" href="#generalized-linear-models-abridged"><span class="octicon octicon-link"></span></a>Generalized linear models, abridged.
</h1>
<p class="view"><a href="https://github.com/bwlewis/GLM">View the Project on GitHub &nbsp;&nbsp;  bwlewis/GLM</a></p>
<a href="mailto:blewis@illposed.net">Bryan W. Lewis</a> and <a href="mailto:michael.kane@yale.edu">Mike Kane</a>
</section>
<section>

<h2>Introduction</h2>

Generalized linear models (GLMs) are indespensible tools in the data science
toolbox. They are applicable to many real-world problems involving continuous,
yes/no, count and survival data. The models themselves are intuitive and can be
used for inference and prediction.  A few very high quality free and open
source sofware implementations are available (in particular within R[7]), as are
several very good commercial ones (Revolution Analytics[8], SAS, Stata).

<p>
<br/>
Despite the wide applicability of GLMs and the availability of high-quality
reference implementations, we've found it hard to find good high-performance
free and open souce software implementations geard to solving large problems.
Moreover, we've had trouble finding succinct references that deal with the core
GLM ideas and implementation details that could help us build our own
high-performance implementations.

<p>

Our goal for this note is to present the core ideas behind GLMs correctly and
also as simply as we can. Focusing on core ideas, we present very basic
implementations, most of which solve GLMs in just a few lines of code.  We
discuss implementation details that enable the development of effective
distributed parallel implementations suitable for solution of large-scale
problems.

<p>

We devlop our ideas in our favorite language, R, but they are easily adapted to
other languages.  Python and Boost/C++ are particularly well-equipped for good
GLM implementations in our opinion.

<h2>Disclaimer</h2>

This is <i>not</i> a formal introduction to GLMs.  We focus on illuminating
just a few key ideas, mostly from a numerical point of view and often focusing
on performance and scalability. We refer the reader to the references [4,6,9]
for more complete introductions to GLMs.

<h2>Linear models</h2>

We begin by defining what we mean when we say "linear model" before moving on
to GLMs. Our presentation and notation generally follow Bj&ouml;rk[1].
<!--
including the following definition of expected value and variance. Let $y$ be
a random variable with distribution function $F(y)$. The expected value $\mu$ and
the variance $\sigma^2$ of $y$ are defined:
\[
E(y) = \mu = \int_{-\infty}^{\infty} ydF(y),
\qquad \sigma^2 = \int_{-\infty}^{\infty}(y-\mu)^2dF(y).
\]
When $y$ is a vector of random variables, we write $E(y)$ to mean the expected
value applied pointwise to each entry, and similarly we define the expected
value of a matrix of random variables to be the expected value of each entry.
Define the variance-covariance matrix of a vector of random variables $y$ with
vector of expected values $\mu$ to be
\[
V(y) = E(yy^T) - \mu \mu^T.
\]
-->
Let $b\in\mathbb{R}^m, b\ne 0$ be a vector of measurements (called a response
vector) and let $A\in\mathbb{R}^{m\times n}$, $\mathrm{rank}(A)=n$, be a matrix
formed from data observations (called the model matrix).  Then
\[
Ax = b + \epsilon
\]
is what we mean when we say "linear model,"
<!--
where $x\in\mathbb{R}^n$ is a
vector of model coefficients and $E(\epsilon)=0$ and $V(\epsilon)=\sigma^2 I$.
That is, the residual error term $\epsilon$ consists of uncorrelated random
variables with zero mean and the same variance. Let's just get phrase out of
the way and assume that the residual error in our models consists of
independent and identically distributed random variables.
-->
where $x\in\mathbb{R}^n$ is a vector of model coefficients and the entries of
the residual error term $\epsilon\in\mathbb{R}^m$ are independent and
identically distributed random variables.  Given a vector $x$ of model
coefficients, we call the product $Ax$ a predictor of the response vector $b$.
We're leaving important definitions of "random variable" and "independent and
identically distributed" to the references.

<p><br/>

Given a model matrix $A$ and response vector $b$, there are many ways we might
go about solving for a vector of model coefficients $x$. We describe a
particularly natural one next.


<h2>Least squares</h2>

One idea we
might think of for solving for the model coefficients would be to compute
the ordinary least squares solution $x$ that minimizes
\[
\min_x ||Ax - b||^2,
\]
where $||\cdot||$ indicates Euclidean norm. Note that the conditions that
$\mathrm{rank}(A)=n$ and $b\ne 0$ imply that there exists a unique least
squares solution.  It turns out that the least squares
solution of linear models has important statistical properties shown in the
seminal work of Gauss[3] and later rediscovered by Markoff[5]. (The least
squares solution defines a <i>minimum variance unbiased estimator</i>, the
technical details of which we leave to the references.)

<h2>Generalized linear models</h2>

Generalized linear models
arise when we constrain the values that the predictor $Ax$ can take on. Such
constraints are important for many practical problems.  For example:
<ul>
<li> The response $b$ may represent binary 0/1 data and we want the predictor $Ax$
to at least be constrained to the interval $[0,1]$
<li> $b$ might represent counts and we desire the predictor to be nonnegative since
negative counts might not make any sense.
</ul>
A standard approach to solving the constrained problem is to transform it into
an unconstrained one by way of a <i>link function</i>. A link function is a
bijection chosen to map the constraint interval onto the real line (the
inverse link function maps the real line onto the constraint interval).
For example, the logistic function
\[
g(a) = \frac{1}{1 + \exp(-a)}
\]
maps arbitrary values $a\in\mathbb{R}$ into the interval $(0,1)$.

<p><br/>
Consider the binary 0/1 data example cited above.  Let
$A\in\mathbb{R}^{m\times n}$ be a given model matrix with $\mathrm{rank}(A)=n$, and let
$b\in\mathbb{R}^m$, $b\ne 0$, be a given response vector. The generalized linear model
problem is to compute an $x$ that satisfies:
\[
Ax = b + \epsilon,\qquad \mathrm{such\, that}\,\, Ax\in(0,1).
\]
We can formulate a related unconstrained problem
that solves for $x$ in
\[
g(Ax) = b + \epsilon,
\]
where $g$ is the logistic function applied elementwise to $Ax$. The problem
is no longer constrained because $g$ maps any value to the interval $(0,1)$.
But it's no longer linear either! How might we solve such a problem? Similarly
to the basic linear model presented earlier, one natural idea is to use least
squares.

<h2>Nonlinear least squares</h2>

Let's quickly recap ideas:
<ul>
<li> The generalized linear model problem is similar to an ordinary linear model problem, but
with constraints on the predictor $Ax$.
<li> We can use a link function to transform a GLM into an unconstrained problem, but...
<li> the resulting unconstrained problem is nonlinear.
</ul>
We can try to use nonlinear least squares to solve the transformed GLM problem
just like we used ordinary least squares to sove
the linear model problem. Consider the binary 0/1 data example in the last section, where
the function $g$ is the logistic function.
Let's cook up a nonlinear least squares method to solve
\[
\min_x ||g(Ax) - b||^2.
\]
We warn you right now that the nonlinear least squares solution turns
out to <i>not</i> be the most efficient way to solve GLMs. Feel free to skip on
down to the maximum likelihood methods below. But this technique is just so
obvious, and we have found GLMs so rarely discussed in this way elsewhere, that
we feel compelled to explore this solution method.

<p>
The following R function solves the GLM problem by direct application of a
nonlinear least squares solver to the link function-transformed problem. We use
R's <tt>optim</tt> optimization function using Broyden's method (a quasi-Newton
solver&mdash;most nonlinear solvers are variations on Newton or secant
methods).

For example, when <tt>g=binomial()$linkinv</tt> (the logistic function), this
roughly corresponds to logistic regression using R's <tt>glm</tt> function.

<pre>
# Input: A is an m by n model matrix, b a response vector of length m,
#        g is the inverse link function (g=binomial()$linkinv  for logistic).
# Output: The model coefficients x.

nnls_glm <- function(A,b,g)
{
  f <- function(x) crossprod(b - g(A %*% x))[]
  optim(rep(0,ncol(A)), fn=f, method="BFGS")$par
}
</pre>
It's an amazingly short function!

<p>

Let's at least check the plausibility of this method on a real world problem,
comparing the result with R's <tt>glm</tt> function.
We use the 'Contraception' data
set from the <tt>mlmRev</tt> R package in favor of a made-up example. You might
need to install the package with <tt>install.packages("mlmRev")</tt> to obtain
these data.  Our examples mostly follow the beautifully concise note on
generalized linear models by Doug Bates[2], which you should read!

<pre>
data("Contraception",package="mlmRev")

# Compute R's glm logistic estimate, using a formula suggested by Doug Bates in
# the note cited above. We also specify x=TRUE, which returns the model matrix
# and response in the model object. We use them as inputs for nnls_glm.

MLE = glm(formula = use ~ age + I(age^2) + urban + livch,
          family = binomial, x=TRUE, data=Contraception)

NNLS = nnls_glm(MLE$x, MLE$y, binomial()$linkinv)

# Compare the model coefficients estimated by each approach:
print(data.frame(MLE=coef(MLE), NNLS=NNLS))

<font color="#00a">
                     MLE         NNLS
(Intercept) -0.949952124 -0.905744758
age          0.004583726  0.007883280
I(age^2)    -0.004286455 -0.004448782
urbanY       0.768097459  0.756306526
livch1       0.783112821  0.763205239
livch2       0.854904050  0.825079534
livch3+      0.806025052  0.753420488
</font>
</pre>
We see that the nonlinear least squares solution is slightly different than R's
<tt>glm</tt> solution (their estimation methods are in fact different).
Later we will compare the two solution methods in a systematic way.


<h2>Maximum likelihood estimation</h2>

You may find it curious that we've formulated generalized linear models so far
without reference to a specific probability distribution for the residual error
values in $\epsilon$. (We <i>have</i> so far assumed that $\epsilon$ consists
of independent and identically distributed random variables.) Equipping
$\epsilon$ with an assumed specific probabilty distribution enables us to
estimate the model coefficients using the method of maxiumum likelihood.

<p>

The maximum likelihood method, in brief, formulates a scalar-valued
<i>likelihood function</i> of the parameters to be estimated and the assumed
joint distribution of the random variables in $\epsilon$. The value of the
likelihood function represents the an estimate of how likely the parameters are
given the data. (If this sounds Bayesian, it is!) One proceeds to then maximize
the likelihood function or equivalently its logarithm, which is simpler in
practice to deal with. <i>Many</i> references (especially [6]) cover the
details of the method, we refer the reader to them for specifics.

<p>

Maximization of a nonlinear likelihood function sounds a lot like the basic
nonlinear least squares approach discussed in the previous section. But it
turns out that the maximum likelihood method converges much faster because it
incorporates information from the assumed distribution into its iterative
solution method, a variation on a method called iteratively reweighted least
square. The following aglorithm presents the key steps of this approach.

<h3>Algorithm IRLS: Iteratively reweighted least squares maximum likelihood GLM estimation</h3>
<h4>Input</h4>
Model matrix $A\in\mathbb{R}^{m\times n}$,
response vector $b\in\mathbb{R}^m$,
inverse link function $g$ and its derivative $g'$,
the variance $\mathrm{var}$ as a function of the mean associated with the distribution function of $\epsilon$,
termination tolerance $\mathrm{tol}$, maximum number of iterations itmax.
Note that "$/$" indicates element-wise division and the inverse link function $g$ and
its derivative and $\mathrm{var}$ are applied element-wise to their vector arguments.
<p>
<ol style="list-style:none">
<li style="margin: 0 0 10px 0;"> Let $x_1=0$.
<li> For $j=1,2,\ldots,\mathrm{itmax}$, do:
<ol style="list-style:none;">
  <li style="margin: 0 0 10px 0;"> Let $\eta = Ax_j$.
  <li style="margin: 0 0 10px 0;"> Let $z = \eta + \frac{b - g(\eta)}{g'(\eta)},\qquad$
  <li style="margin: 0 0 10px 0;"> Let $W = \mathrm{diag}(g'(\eta)^2/\mathrm{var}(g(\eta)))$.
  <li style="margin: 0 0 10px 0;"> Let $x_{j+1} = x_j + (A^T W A)^{-1} A^T W z$.
  <li> Stop if $||x_{j+1} - x_j|| \lt \mathrm{tol}$.
</ol>
</ol>
<hr>
It's remarkable how rapidly this simple algorithm converges. Before we dive in to
some specific details, let's look at an R implementation of this algorithm and
compare it to R's <tt>glm</tt> function.
<p>
<pre>
minimalist_glm =
function(A, b, family=binomial, maxit=25, tol=1e-08)
{
  x = rep(0,ncol(A))
  for(j in 1:maxit)
  {
    eta    = A %*% x
    g      = family()$linkinv(eta)
    gprime = family()$mu.eta(eta)
    z      = eta + (b - g) / gprime
    W      = as.vector(gprime^2 / family()$variance(g))
    ATWA   = crossprod(A,(W * A))
    ATWz   = t(crossprod(W*z,A))
    xold   = x
    x      = solve(ATWA, ATWz, tol=2*.Machine$double.eps)
    if(sqrt(crossprod(x-xold)) &lt; tol) break
  }
  list(coefficients=x,iterations=j)
}
</pre>
Let's compare our minimalist estimator to R's <tt>glm</tt> function using the
same example that we used to compare the nonnegative least squares method:
<p>
<pre>
data("Contraception",package="mlmRev")

# Model estimated with R's glm function, returning model matrix and response
# in $x and $y, respectively:
MLE = glm(formula = use ~ age + I(age^2) + urban + livch,
          family = binomial, x=TRUE, data=Contraception)

# Model estimated with our radically stripped-down minimalist implementation:
mini = minimalist_glm(MLE$x, MLE$y, family=binomial)

print(data.frame(MLE=coef(MLE), minimalist=coef(mini)))

<font color="#00a">
                     MLE   minimalist
(Intercept) -0.949952124 -0.949952124
age          0.004583726  0.004583726
I(age^2)    -0.004286455 -0.004286455
urbanY       0.768097459  0.768097459
livch1       0.783112821  0.783112821
livch2       0.854904050  0.854904050
livch3+      0.806025052  0.806025052
</font>
</pre>
The results are the same to displayed accuracy. Our minimalist implementation
converged after only five iterations in this example.

<h2>IRLS Notes</h2>

Looking at algorithm IRLS, a few potential issues are apparent. What if
$g'(\eta) \approx 0$ at some point? Or similarly, what if
$\mathrm{var}(g(\eta))\approx 0$ or $W$ is such that $A^TWA$ becomes singular
or extremely ill-conditioned?  Any of these cases lead to break down of our
simple implementation. In practice for most values of link function and
variance function ill-conditioning in any of these problem areas leads
to non-convergence of the method.

<p>

Careful implementations include numeric checks and emit associated warnings as
the algorithm proceeds. Ill-conditioning of the weighted model matrix is
perhaps the most serious potential problem because small errors in the response
vector can lead to huge differences in the solution model coefficients.
Most implementations compute
\[
(A^T W A)^{-1} A^T W z
\]
by way of a signular value or rank-revealing QR decomposition of $A$ that detects
numerically rank-deficient or extremely ill-conditioned problems. When such
a problem occurs, it can usually be resolved by reducing the number of variables
in the model.


<p>

Note that when the link function is the identity function and the error
distribution function of $\epsilon$ is Gaussian with mean zero and variance
one, then algorithm IRLS becomes:

<ol style="list-style:none">
<li style="margin: 0 0 10px 0;"> $x_1=0$
<li style="margin: 0 0 10px 0;"> $\eta = Ax_1 = 0$
<li style="margin: 0 0 10px 0;"> $z = \eta + \frac{b - g(\eta)}{g'(\eta)} = \frac{b - 0}{1} = b$
<li style="margin: 0 0 10px 0;"> $W = \mathrm{diag}(g'(\eta)^2/\mathrm{var}(g(\eta))) = \mathrm{diag}(1^2 /1)= I$
<li style="margin: 0 0 10px 0;"> $x_{2} = 0 + (A^T A)^{-1} A^T b$,
</ol>

which is just ordinary least squares.

<h2>IRLS Performance considerations</h2>

<h2>Distributed parallel computation of IRLS</h2>


<h2>Footnote: Comparing NNLS and IRLS methods</h2>

</section>

<section>
<h2>References</h2>

<ol>
<li> Bj&ouml;rck, &Aring;.,  Numerical Methods for Least Squares Problems, SIAM, Philadelphia, 1996.
<li> Bates, D., <a href="http://www.stat.wisc.edu/courses/st849-bates/lectures/GLMH.pdf">http://www.stat.wisc.edu/courses/st849-bates/lectures/GLMH.pdf</a>.
<li> Gauss, C. F., Theoria combinationis observationum erroribus minimis obnoxiae, Pars prior, 1863 (1st written 1821).
<li> Hastie, T. J. and Pregibon, D., Generalized linear models,
     Chapter 6 of Statistical Models in S, eds J. M. Chambers and T.
     J. Hastie, Wadsworth &amp; Brooks/Cole, 1992.
<li> Markoff, A., Wahrscheinlichheitsrechnug,  Leipzig, 1912.
<li> McCullagh P. and Nelder, J. A., Generalized Linear Models, Chapman and Hall, London 1989.
<li> The R project <a href="http://www.r-project.org">http://www.r-project.org</a>.
<li> Revolution Analytics <a href="http://revolutionanalytics.com">http://revolutionanalytics.com</a>.
<li> Zelen, M., Linear estimation and relted topics, in Survey of Numerical Analysis, Todd. J. ed, McGraw-Hill, New York, 1962, pp. 429-441.
</ol>
</secion>
</div>

<script src="javascripts/scale.fix.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</body>
</html>
