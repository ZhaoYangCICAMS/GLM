<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>GLMs, abridged</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
<section>
<h1>
<a name="generalized-linear-models-abridged" class="anchor" href="#generalized-linear-models-abridged"></a>
Generalized linear models, abridged.
</h1>
<p class="view"><a href="https://github.com/bwlewis/GLM">View the Project on GitHub &nbsp;&nbsp;  bwlewis/GLM</a></p>
<a href="mailto:michael.kane@yale.edu">Mike Kane</a>
and
<a href="mailto:blewis@illposed.net">Bryan W. Lewis</a>
</section>
<section>

<h2>Introduction</h2>
<p>
Generalized linear models (GLMs) are indispensable tools in the data science
toolbox. They are applicable to many real-world problems involving continuous,
yes/no, count and survival data. The models themselves are intuitive and can be
used for inference and prediction.  A few very high quality free and open
source software implementations are available (in particular within R[10]), as are
several very good commercial ones (Revolution Analytics[11], SAS, Stata).

<p>
Despite the wide applicability of GLMs and the availability of high-quality
reference implementations, we've found it hard to find good high-performance
free and open source software implementations geared to solving large problems.
Moreover, we've had trouble finding succinct references that deal with the core
GLM ideas and implementation details that could help us build our own
high-performance implementations.

<p>

Our goal for this note is to present the core ideas behind GLMs We present very
basic implementations, most of which solve GLMs in just a few lines of code.
We discuss implementation details that enable the development of effective
distributed parallel implementations suitable for solution of large-scale
problems.

<p>

We develop our ideas in our favorite language, R, but they are easily adapted to
other languages.  Python and Boost/C++ are particularly well-equipped for good
GLM implementations in our opinion.

<h2>Disclaimer</h2>
<p>
This is <i>not</i> a formal introduction to GLMs.  We focus on illuminating
just a few key ideas, mostly from a numerical point of view and often focusing
on performance and scalability. We refer the reader to the references [3,5,7,9,12]
for complete introductions to linear models.

<p>

These are working notes. We'll continue to tweak and revise them.

<h2>Linear models</h2>
<p>
We begin by defining what we mean when we say "linear model" before moving on
to GLMs. Our presentation and notation generally follow Bj&ouml;rk[2].
Let $b\in\mathbb{R}^m, b\ne 0$ be a vector of measurements (called a response
vector) and let $A\in\mathbb{R}^{m\times n}$, $\mathrm{rank}(A)=n$, be a matrix
formed from data observations (called the model matrix).  Then
\[
Ax = b + e
\]
is what we mean when we say "linear model,"
where $x\in\mathbb{R}^n$ is a vector of model coefficients and the entries of
the residual error term $e\in\mathbb{R}^m$ are independent and
identically distributed random variables.  Given a vector $x$ of model
coefficients, we call the product $Ax$ a predictor of the response vector $b$.
We're leaving important definitions of "random variable" and "independent and
identically distributed" to the references.

<p>

Given a model matrix $A$ and response vector $b$, there are many ways we might
go about solving for a vector of model coefficients $x$. We describe a
particularly natural one next.


<h2>Least squares</h2>

One idea we
might think of for solving for the model coefficients would be to compute
the ordinary least squares solution $x$ that minimizes
\[
\min_x ||Ax - b||^2,
\]
where $||\cdot||$ indicates Euclidean norm. Note that the conditions that
$\mathrm{rank}(A)=n$ and $b\ne 0$ imply that there exists a unique least
squares solution.  It turns out that the least squares
solution of linear models has important statistical properties shown in the
seminal work of Gauss[4] and later rediscovered by Markoff[6]. (The least
squares solution defines a <i>minimum variance unbiased estimator</i>, the
technical details of which we leave to the references, in particular see [5,7].)

<h2>Generalized linear models</h2>
<p>
Relaxing conditions on linear models by allowing the entries of $e$ to be
correlated or to be distributed with differing variances gives us generalized
linear models. We need a few technical definitions to make this precise.

<p>

Let $\epsilon$ be
a random variable with distribution function $F(\epsilon)$. The expected value $\mu$ and
the variance $\sigma^2$ of $\epsilon$ are defined as:
\[
E(\epsilon) = \mu = \int_{-\infty}^{\infty} \epsilon dF(\epsilon),
\qquad \sigma^2 = \int_{-\infty}^{\infty}(\epsilon-\mu)^2dF(\epsilon).
\]
When $e$ is a vector of random variables, we write $E(e)$ to mean the expected
value of each entry, and similarly we define the expected value of a matrix of
random variables to be the expected value of each entry.  Define the
variance-covariance matrix of a vector of random variables $e$ with mean vector
$\mu$ to be
\[
V(e) = E(ee^T) - \mu \mu^T.
\]

<p>

The assumption of independence and identical distribution among the random variables
in the error term $e$ of our earlier linear model required that $V(e) = \sigma^2 I$.
General Gauss-Markoff linear models relax this requirement. When we say "generalized
linear model" we mean a linear model of the form:
\[
Ax = b + e,\qquad V(e)=\sigma^2W^{-1},
\]
where $A$ is an $m\times n$ matrix of $\mathrm{rank}(A)=n$,
$e$ is a vector of random variables with zero mean and covariance matrix
$\sigma^2 W^{-1}$ and $W$ is a symmetric positive definite matrix.
See Bk&ouml;rk[2, pp. 160-165]
for a concise introduction to generalized linear models that we largely follow here.
The solution of the generalized linear model can be formulated as
\[
\min_x(Ax-b)^TW(Ax -b),
\]
with solution $x=(A^T W A)^{-1}A^T W b$.
The numerical solution of model problems of this form was carefully
analyzed by Paige[9].
We focus on simplified generalized models here
and assume that $W$ is a diagonal
matrix with positive entries along its diagonal.

<h2>Generalized linear models in practice</h2>
<p>
Generalized linear models typically
arise when we constrain the values that the predictor $Ax$ can take on and assume
a specific distribution function for the entries of $e$.
Constraints are important for many practical problems.  For example:
<ul>
<li> The response $b$ may represent binary 0/1 data and we want the predictor $Ax$
to at least be constrained to the interval $[0,1]$, in which case we might assume,
for example, a binomial distribution function for $e$.
<li> $b$ might represent counts and we desire the predictor to be nonnegative since
negative counts might not make any sense. In this case we might assume a
Poisson distribution function.
</ul>
A standard approach to solving the constrained problem is to transform it into
an unconstrained one by way of a <i>link function</i>. A link function is a
bijection chosen to map the constraint interval onto the real line (the
inverse link function maps the real line onto the constraint interval).
For example, the logistic function
\[
g(a) = \frac{1}{1 + \exp(-a)}
\]
maps arbitrary values $a\in\mathbb{R}$ into the interval $(0,1)$.

<p>
Consider the binary 0/1 data example cited above.  Let
$A\in\mathbb{R}^{m\times n}$ be a given model matrix with $\mathrm{rank}(A)=n$, and let
$b\in\mathbb{R}^m$, $b\ne 0$, be a given response vector. The generalized linear model
problem is to compute an $x$ that satisfies:
\[
Ax = b + e,\qquad \mathrm{such\, that}\,\, Ax\in(0,1).
\]
We can formulate a related unconstrained problem
that solves for $x$ in
\[
g(Ax) = b + e,
\]
where $g$ is the logistic function applied elementwise to $Ax$. The problem
is no longer constrained because $g$ maps any value to the interval $(0,1)$.
But it's no longer linear either!

<p>
The choice of the nature of the error distribution defines the norm-weighting
matrix $W$ discussed in the previous section. If we assume that the entries of
$e$ consist of uncorrelated random variables with identical variance, then
$W=\sigma^{-2}I$ and the problem reduces to simple nonlinear least squares:
\[
\min_x ||g(Ax) - b||^2.
\]
That problem can be solved in R by several optimization methods,
including Broyden's quasi-Newton method as follows:
<pre>
# Input: A is an m by n model matrix, b a response vector of length m,
#        g is the inverse link function.
# Output: The model coefficients x.

nnls_glm = function(A,b,g)
{
  f = function(x) crossprod(b - g(A %*% x))[]
  optim(rep(0,ncol(A)), fn=f, method="BFGS")$par
}
</pre>

If the entries of $e$ consist of uncorrelated random variables but with
different variances, then $W\ne I$ but $W$ is still a diagonal matrix. This is
common to many generalized linear model problems and results in a weighted
nonlinear least squares problem (compare to the generalized linear model definition
in the previous section):
\[
\min_x(g(Ax)-b)^TW(g(Ax) -b),
\]
typically solved by the iteratively reweighted least square method described next.

<h3>
<a name="irls" class="anchor" href="#irls"></a>
Algorithm IRLS: Iteratively reweighted least squares estimation
</h3>
<h4>Input</h4>
Model matrix $A\in\mathbb{R}^{m\times n}$,
response vector $b\in\mathbb{R}^m$,
inverse link function $g$ and its derivative $g'$,
the variance $\mathrm{var}$ as a function of the mean associated with the
distribution function of $e$,
termination tolerance $\mathrm{tol}$, maximum number of iterations itmax.
Here and below, "$/$" indicates element-wise division and the inverse
link function $g$ and its derivative and $\mathrm{var}$ are applied
element-wise to their vector arguments.
<p>
<ol style="list-style:none">
<li class="lino"> Let $x_1=0$.
<li> For $j=1,2,\ldots,\mathrm{itmax}$, do:
<ol style="list-style:none;">
  <li class="lino"> Let $\eta = Ax_j$.
  <li class="lino"> Let $z = \eta + \frac{b - g(\eta)}{g'(\eta)},\qquad$
  <li class="lino"> Let $W = \mathrm{diag}(g'(\eta)^2/\mathrm{var}(g(\eta)))$.
  <li class="lino"> Let $x_{j+1} = x_j + (A^T W A)^{-1} A^T W z$.
  <li> Stop if $||x_{j+1} - x_j|| \lt \mathrm{tol}$.
</ol>
</ol>
<h4>Output</h4>
Model coefficients $x_{j+1}$.
<br/>
<hr>
<p>
It's remarkable how rapidly this simple algorithm converges. Before we dive in to
details, let's look at one simple R implementation of this algorithm and
compare it to R's <tt>glm</tt> function. The minimalist GLM function below
(closer to R's <tt>glm.fit</tt>, really)
takes as input a model matrix $A$, a response vector $b$, an R distribution family
object, and optional maximum number of iterations and tolerance values.
(This is the shortest GLM implementation we could come up with!)
<p>
<pre>
irls =
function(A, b, family=binomial, maxit=25, tol=1e-08)
{
  x = rep(0,ncol(A))
  for(j in 1:maxit)
  {
    eta    = A %*% x
    g      = family()$linkinv(eta)
    gprime = family()$mu.eta(eta)
    z      = eta + (b - g) / gprime
    W      = as.vector(gprime^2 / family()$variance(g))
    xold   = x
    x      = solve(crossprod(A,W*A), crossprod(A,W*z), tol=2*.Machine$double.eps)
    if(sqrt(crossprod(x-xold)) &lt; tol) break
  }
  list(coefficients=x,iterations=j)
}
</pre>
<p>
Let's compare our super-minimalist IRLS estimator to R's <tt>glm</tt> function.
We use a real-world data example from the "mlmRev" package, which you may
need to install with <tt>install.packages("mlmRev")</tt>. The example
is described in Doug Bates' beautifully concise notes on GLMs[3] (you
should read them!).
<pre>
data("Contraception",package="mlmRev")

# Model estimated with R's glm function, returning model matrix and response
# in $x and $y, respectively:
R_GLM = glm(formula = use ~ age + I(age^2) + urban + livch,
            family = binomial, x=TRUE, data=Contraception)

# Model estimated with our radically stripped-down minimalist implementation:
mini = irls(R_GLM$x, R_GLM$y, family=binomial)

print(data.frame(R_GLM=coef(R_GLM), minimalist=coef(mini)))
<font color="#00a">
                   R_GLM   minimalist
(Intercept) -0.949952124 -0.949952124
age          0.004583726  0.004583726
I(age^2)    -0.004286455 -0.004286455
urbanY       0.768097459  0.768097459
livch1       0.783112821  0.783112821
livch2       0.854904050  0.854904050
livch3+      0.806025052  0.806025052
</font>
</pre>
The results agree to displayed accuracy. Our minimalist implementation
of IRLS converged after five iterations in this example.

<h2>IRLS Notes</h2>
<p>
IRLS solves a generalized linear model in each iteration:
\[
(A^T W A)^{-1} A^T W z,
\]
which is the just the solution to the (weighted) least squares problem:
\[
\min||W^{1/2}(Ax - b)||^2,
\]
so long as the diagonal weight matrix $W$ consists of positive entries along
the diagonal.  We make this a requirement of IRLS. Looking at how the weight
matrix $W$ is defined above, we see that this means that the derivative of the
inverse link function $g'$ applied to $Ax_j$ must never vanish. Note that
in the logistic regression case $g'(a) = g(a)(1-g(a))$ is <i>never</i> zero.

<p>

Clearly, some other numerical difficulties can arise.  What if 
$g'(\eta) \approx 0$ at some point? Or similarly, what if
$\mathrm{var}(g(\eta))\approx 0$
or $W$ is such that $A^TWA$ becomes extremely ill-conditioned?  A more
subtle problem occurs when the maximum ratio of any two entries along the
diagonal of $W$ is a big number.  Such problems are called <i>stiff</i> and
bring their own set of numerical difficulties.

<p>

Bj&ouml;rk[2, (pp. 165&mdash;171)] presents an excellent survey of solution
approaches for the weighted least squares problem.  Direct solution of the
normal equations (as illustrated in our simple R implementation of the IRLS
algorithm above) is potentially numerically unstable and not generally advised.
The brief survey in Bj&ouml;rk is really fascinating reading. His cited references
show us that even standard Household QR decomposition can give poor accuracy
for stiff problems unless it's modified to include column <i>and</i> row
pivoting. It turns out that the simplest numerically stable implementation for
solving the linear system within the IRLS method is the Givens QR algorithm,
shown by Anda and Park[1].
 
<p>

In practice, with judicious monitoring of the entries of $W$ (in particular the
stiffness $\max_j(W_{j,j})/\min_j(W_{j,j})$) and the quantities $g'(\eta)$ and
$\mathrm{var}(g(\eta))$, we can choose among the most stable and convenient
solution methods and simply warn the user when the algorithm gets into trouble
numerically. This is indeed what R does in its own <tt>glm.fit</tt>
implementation, which uses a Household QR decomposition of $WA$ with column
pivoting, modified to reveal the rank of the matrix at each step.


<h3>Connection to ordinary least squares</h3>
<p>
When the link function is the identity function and the error
distribution function of $e$ is Gaussian with mean zero and variance
one, then the IRLS algorithm becomes:

<ol style="list-style:none">
<li class="lino"> $x_1=0$
<li class="lino"> $\eta = Ax_1 = 0$
<li class="lino"> $z = \eta + \frac{b - g(\eta)}{g'(\eta)} = 0 + \frac{b - 0}{1} = b$
<li class="lino"> $W = \mathrm{diag}(g'(\eta)^2/\mathrm{var}(g(\eta))) = \mathrm{diag}(1^2 /1)= I$
<li class="lino"> $x_{2} = 0 + (A^T A)^{-1} A^T b$,
</ol>

which is just the ordinary least squares solution.

<h2>Performance and stability</h2>

<p>

We already sketched a simple implementation of the IRLS algorithm above (see <a
href="#irls">IRLS</a>). Recall that that method can suffer from potential
numerical instability.  We consider variations of that method in this section
that focus on numerical stability and performance.

<p>
Perhaps the most important performance considerations for a good GLM/IRLS
implementation are:

<ol>
<li> Take advantage of fast basic linear algebra subroutine (BLAS) and
     LAPACK libraries whenever possible.
<li> Take advantage of fast sparse matrix vector products.
</ol>
We will see that #2 is somewhat at odds with numerical stability. We seek to develop a
flexible method that can reasonably accommodate stability and performance.

<h3>
<a name="qrnewton" class="anchor" href="#qrnewton"></a>
Algorithm IRLS (QR Newton variant)
</h3>
<p>
This method was defined by O'Leary[8]. The idea is to compute a QR
factorization of the model matrix $A$ just once, possibly by a rank-revealing
QR method to identify problematic model matrices at the start. The
factorization is then used in the IRLS iteration.

<p>
It turns out that IRLS iterations that rely on factored forms of the model
matrix can be made more computationally stable and efficient by updating the
residual error vector instead of the model coefficients. The idea is to let
$r = b - Ax$ for some initial guess of $x$, iteratively update $r$ by IRLS,
and then solve the consistent linear system $Ax = b - r$ for $x$. Here is the
method:

<h4>Input</h4>
Model matrix $A\in\mathbb{R}^{m\times n}$,
response vector $b\in\mathbb{R}^m$,
inverse link function $g$ and its derivative $g'$,
the variance $\mathrm{var}$ as a function of the mean associated with the
distribution function of $e$,
termination tolerance $\mathrm{tol}$, maximum number of iterations itmax.
We also requre a sense of "too small" for the numerical checks indicated
in the algorithm.
<p>
<ol style="list-style:none">
<li class="lino"> Let $r=b$ (or possibly some other initial guess).
<li class="lino"> Compute $A = QR$.
<li> For $j=1,2,\ldots,\mathrm{itmax}$, do:
<ol style="list-style:none;">
  <li class="lino"> Let $\eta = b - r$.
  <li class="lino"> Let $z = \eta + \frac{b - g(\eta)}{g'(\eta)},\qquad$
  <li class="lino"> Let $W = \mathrm{diag}(g'(\eta)^2/\mathrm{var}(g(\eta)))$.
  <li class="lino"> If $\min(\mathrm{diag}(W))$ is too small error out or warn the user.
  <li class="lino"> Let $r_{old} = r$.
  <li class="lino"> Let $r = b - A(A^T W A)^{-1} A^T W z = b - Q(Q^T W Q)^{-1}Q^T W z.$
  <li class="lino"> (Solve with a Cholesky factorization of $Q^T W Q$.)
  <li> Exit for loop if $||r_{old} - r|| \lt \mathrm{tol}$.
</ol>
<li class="lino"> Solve $R x = Q^T(b - r)$ for $x$ (simple backsolve).
</ol>
<h4>Output</h4>
Model coefficients $x$.
<p>
<h4>R implementation</h4>
<pre>
irls_qrnewton =
function(A, b, family=binomial, maxit=25, tol=1e-08)
{
  r  = b
  QR = qr(A)
  Q  = qr.Q(QR)
  R  = qr.R(QR)
  for(j in 1:maxit)
  {
    eta    = b - r
    g      = family()$linkinv(eta)
    gprime = family()$mu.eta(eta)
    z      = eta + (b - g) / gprime
    W      = as.vector(gprime^2 / family()$variance(g))
    wmin   = min(W)
    if(wmin < sqrt(.Machine$double.eps))
      warning("Tiny weights encountered, likely numerical problems!")
    rold   = r
    C   = chol(crossprod(Q, W*Q))
    s   = forwardsolve(t(C), crossprod(Q,W*z))
    s   = backsolve(C,s)
    r      = b - Q %*% s
    if(sqrt(crossprod(r-rold)) < tol) break
  }
  x = backsolve(R, crossprod(Q,b-r))
  list(coefficients=x,iterations=j)
}
</pre>
<hr>
<p>
This is a pretty cool IRLS implementation. It avoids direct solution of the
normal equations in each step by only using the orthonormal $Q$ matrix and
the current set of weights. That results in potentially much better numerical
stability than our earlier simple implementation <a href="#irls">IRLS</a>.
Positive-definiteness for the Cholesky factorization is easily checked in
each iteration by examining the diagonal entries of the weight matrix.

<p>

The computations rely on the QR and Cholesky factorizations and matrix
multiplication, all available to R from high-performance tuned BLAS and LAPACK
libraries like the Intel MKL, AMD ACML, OpenBlas (Goto), or ATLAS.  That means
that if you have one of those BLAS libraries installed and linked to R, then
this method will take advantage of available CPU vectorized instrucions and
multithreading and run fast.

<p>

R's native IRLS method embedded in its <tt>glm.fit</tt> function computes
a QR decomposition of the weighted matrix $WA$ in each iteration step.
O'Leary's <a href="#qrnewton">QR Newton method</a> shown above instead
only computes one QR factorization of $A$ at the start. It then monitors
possible rank deficiency by checking the size of the smallest diagonal
element of the weight matrix in each step.

<p>
What if the final system $Ax = b -r$ becomes inconsistent due to roundoff or
other errors? That is certainly one potential numerical issue with the
method. See O'Leary[8] for a brief discussion of
this. In practice the method is numerically robust even if this occurs, although
the residual norm $||Ax - b+r||$ is a good diagnostic value for the method.

<p>
Let's at least verify the plausibility of this method by repeating an earlier
example from Doug Bates[3]:
<pre>
data("Contraception",package="mlmRev")

# Model estimated with R's glm function, returning model matrix and response
# in $x and $y, respectively:
R_GLM = glm(formula = use ~ age + I(age^2) + urban + livch,
            family = binomial, x=TRUE, data=Contraception)

# Model estimated by IRLS/QR Newton:
iqrn = irls_qrnewton(R_GLM$x, R_GLM$y, family=binomial)

print(data.frame(R_GLM=coef(R_GLM), qr_newton=coef(iqrn)))
<font color="#00a">
                   R_GLM    qr_newton
(Intercept) -0.949952124 -0.949952124
age          0.004583726  0.004583726
I(age^2)    -0.004286455 -0.004286455
urbanY       0.768097459  0.768097459
livch1       0.783112821  0.783112821
livch2       0.854904050  0.854904050
livch3+      0.806025052  0.806025052
</font>
</pre>
No surprises here for this modest-sized and well-conditioned data set. The
IRLS/QR Newton method produces coefficients that match those of R's native GLM
method to displayed accuracy.



<h3>
<a name="svdnewton" class="anchor" href="#svdnewton"></a>
Algorithm IRLS (SVD Newton variant)
</h3>
<p>
If you're really concerned about rank deficiency of the model matrix and you
don't have a rank-revealing QR method available and you're willing to pay a
few extra flops on the initial matrix decomposition, then the IRLS QR Newton
variation can be easily adapted to an SVD-based method that definitively
determines the rank of the model matrix.

<h4>Input</h4>
Model matrix $A\in\mathbb{R}^{m\times n},\,\,m\le n$,
response vector $b\in\mathbb{R}^m$,
inverse link function $g$ and its derivative $g'$,
the variance $\mathrm{var}$ as a function of the mean associated with the
distribution function of $e$,
termination tolerance $\mathrm{tol}$, maximum number of iterations itmax.
<p>
<ol style="list-style:none">
<li class="lino"> Let $r=b$ (or possibly some other initial guess).
<li class="lino"> Compute the SVD
$AV = U\Sigma,\,\,\,\, U^TU = I,\,\,\, V^TV = I, \,\,\,\Sigma=\mathrm{diag}(\sigma_1\ge\sigma_2\ge\cdots\sigma_m\gt 0)$.
<li class="lino">(If any $\sigma_j=0$ then $A$ is rank-deficient, stop and warn the user.)
<li> For $j=1,2,\ldots,\mathrm{itmax}$, do:
<ol style="list-style:none;">
  <li class="lino"> Let $\eta = b - r$.
  <li class="lino"> Let $z = \eta + \frac{b - g(\eta)}{g'(\eta)},\qquad$
  <li class="lino"> Let $W = \mathrm{diag}(g'(\eta)^2/\mathrm{var}(g(\eta)))$.
  <li class="lino"> If $\min(\mathrm{diag}(W))$ is too small error out or warn the user.
  <li class="lino"> Let $r_{old} = r$.
  <li class="lino"> Let $r = b - U(U^T W U)^{-1}U^T W z.$
  <li class="lino"> (Solve with a Cholesky factorization of $U^T W U$.)
  <li> Exit for loop if $||r_{old} - r|| \lt \mathrm{tol}$.
</ol>
<li class="lino"> Let $x = V\Sigma^{-1}U^T(b - r)$.
</ol>
<h4>Output</h4>
Model coefficients $x$.
<hr/>
<p>

Similarly to the QR Newton method listed above, the SVD-based method only uses
the orthogonal $U$ matrix and the current weighting matrix in each iteration.
A side-benefit of the SVD is that the final solution for model coefficients
no longer requires a backsolve. This is Bryan's personal favorite IRLS
implementation. An R implementation of this algorithm is available in the
<tt>implementations.R</tt> file in the Github repository.


<h3>
<a name="sparse" class="anchor" href="#sparse"></a>
Sparse model matrices
</h3>

<p>

Sparse model matrices arise in many practical problems, in particular when
categorical variables are encoded by a unit basis to form so-called treatment
contrasts. (The categories are contrasted against a baseline category.)
We would like to take advantage of sparsity in the model matrix when it occurs,
both to reduce its storage requirements and to improve performance of the
IRLS method. This is particularly important for large problems.

<p>

The QR- and SVD-based IRLS methods described above destroy sparsity in the
model matrix by replacing it with dense factors $Q$ and $U$, respectively.
Although high-performance BLAS and LAPACK implementations are available for
each method, neither takes any advantage of sparse model matrices.

<p>

Let's set aside numerical stability concerns for a moment and investigate how
we might implement our original simple <a href="#irls">IRLS</a> method to take
advantage of sparse model matrices.

<p>

In practice, we are likely to encounter problems with a mixture of continuous
and categorial variables that lead to model matrices with some dense columns
and some sparse columns. For example, the model matrix from the mlmRev package
used in the previous examples has two columns with real-valued (dense) data and
five columns containing only ones and zeros (sparse).

<p>

A large portion of the computational work in the <a href="#irls">IRLS</a>
method occurs in the matrix product $A^TWA$. We want to compute this product as
efficiently as possible. That means using fast optimized BLAS methods for the
dense part, and fast sparse matrix multiplication methods for the sparse part,
suggesting that we permute the columns of the model matrix and split it into
dense and sparse parts. See the <tt>sparse_n_dense.R</tt> file available in
this project's Github repository for an example microbenchmark and related
numerical experiments that investigates this.

<p>

The performance change realized from splitting the matrix product into sparse
and dense components compared to the usual dense computation is sensitive to
many factors, including the model matrix size and ratio of sparse to dense
columns, the CPU architecture and number of cores, and BLAS library used.  We
list some results computed on our workstation using the
<tt>sparse_n_dense.R</tt> experiment below for reference (don't take the
results too seriously, instead experiment on your own!).

<p>

Our test workstation consisted of  one AMD A10-7850K CPU with 4 physical cores
and 16 GB DDR3 RAM, running Ubuntu 12.04 and R version 3.0.2. We could only get
Ubuntu's default <tt>libopenblas</tt> library to use two cores, even after
trying many suggested environment variable settings. But the AMD ACML BLAS
library we used was version 5.3.1 with fused multiply/add and multicore
support, and it is probably the highest performance BLAS library available for
this CPU.  The test matrix consisted of 100000 rows and 1000 columns, 100 of
them dense and 900 of them sparse with 1% about fill-in, all randomly
generated. See the <tt>sparse_n_dense.R</tt> file for details.

<table>
<th>BLAS library<th>Dense crossprod time in seconds<th>Split crossprod time in seconds
<tr><td>ACML FMA4_MP BLAS<td>3.0<td>1.6
<tr><td>OpenBLAS<td>4.7<td>1.6
</table>
Remember that this experiment is very sensitive to many factors. We encourage
you to experiment on your own.


<h4>Numerical stability and sparse problems</h4>
<p>

We've seen one way to adapt our basic <a href="#irls">IRLS</a> method to take
advantage of sparse model matrices for performance, but what about numerical
stability?  Although we can't match the numerical properties of the QR- or
SVD-based methods, all is not lost. We can apply a rank-revealing Cholesky
decomposition to the product $A^TWA$ inside each IRLS iteration, for example
using the LAPACK <tt>dpstrf</tt> routine. That Cholesky decomposition uses
column pivoting and can detect rank deficiency at each step of the algorithm.
That way we can at least detect possible problems during the iteration
(although we are still subject to loss of numerical accuracy from very
ill-conditioned or stiff problems). The LAPACK <tt>dpstrf</tt> routine is
directly available from R's <tt>chol</tt> function.

<p>

Let's put this long-winded sparse discussion together into a practical
algorithm:




<h2>Breaking large GLMs into a series of smaller problems</h2>

<h2>Distributed parallel computation of IRLS</h2>


</section>

<section>
<h2>References</h2>

<ol>
<li> Anda, A. and Park, H., Self-scaling fast rotations for stiff least squares problems, Lin. Alg. Appl., 234, 1996, pp. 137-162.
<li> Bj&ouml;rck, &Aring;.,  Numerical Methods for Least Squares Problems, SIAM, Philadelphia, 1996.
<li> Bates, D., <a href="http://www.stat.wisc.edu/courses/st849-bates/lectures/GLMH.pdf">http://www.stat.wisc.edu/courses/st849-bates/lectures/GLMH.pdf</a>.
<li> Gauss, C. F., Theoria combinationis observationum erroribus minimis obnoxiae, Pars prior, 1863 (1st written 1821).
<li> Hastie, T. J. and Pregibon, D., Generalized linear models,
     Chapter 6 of Statistical Models in S, eds J. M. Chambers and T.
     J. Hastie, Wadsworth &amp; Brooks/Cole, 1992.
<li> Markoff, A., Wahrscheinlichheitsrechnug,  Leipzig, 1912.
<li> McCullagh P. and Nelder, J. A., Generalized Linear Models, Chapman and Hall, London 1989.
<li> O'Leary, D., Robust regression computation using iteratively reweighted least squares, Siam J. Mat. Anal. Appl., Vol. 11 No. 3, 1990, pp. 466-480.
<li> Paige, C. C., Fast numerically stable computations for generalized least squares problems, Siam J. Num. Anal., 16, 1979, pp. 165-171.
<li> The R project <a href="http://www.r-project.org">http://www.r-project.org</a>.
<li> Revolution Analytics <a href="http://revolutionanalytics.com">http://revolutionanalytics.com</a>.
<li> Zelen, M., Linear estimation and relted topics, in Survey of Numerical Analysis, Todd. J. ed, McGraw-Hill, New York, 1962, pp. 429-441.
</ol>
</secion>
</div>

<script src="javascripts/scale.fix.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
"HTML-CSS": { scale: 90}
});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</body>
</html>
