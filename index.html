<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>GLMs, abridged</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
<section>
<h1>
<a name="generalized-linear-models-abridged" class="anchor" href="#generalized-linear-models-abridged"><span class="octicon octicon-link"></span></a>
Generalized linear models, abridged.
</h1>
<p class="view"><a href="https://github.com/bwlewis/GLM">View the Project on GitHub &nbsp;&nbsp;  bwlewis/GLM</a></p>
<a href="mailto:michael.kane@yale.edu">Mike Kane</a>
and
<a href="mailto:blewis@illposed.net">Bryan W. Lewis</a>
</section>
<section>

<h2>Introduction</h2>

Generalized linear models (GLMs) are indispensable tools in the data science
toolbox. They are applicable to many real-world problems involving continuous,
yes/no, count and survival data. The models themselves are intuitive and can be
used for inference and prediction.  A few very high quality free and open
source software implementations are available (in particular within R[10]), as are
several very good commercial ones (Revolution Analytics[11], SAS, Stata).

<p>
<br/>
Despite the wide applicability of GLMs and the availability of high-quality
reference implementations, we've found it hard to find good high-performance
free and open source software implementations geared to solving large problems.
Moreover, we've had trouble finding succinct references that deal with the core
GLM ideas and implementation details that could help us build our own
high-performance implementations.

<p>

Our goal for this note is to present the core ideas behind GLMs correctly and
also as simply as we can. Focusing on core ideas, we present very basic
implementations, most of which solve GLMs in just a few lines of code.  We
discuss implementation details that enable the development of effective
distributed parallel implementations suitable for solution of large-scale
problems.

<p>

We develop our ideas in our favorite language, R, but they are easily adapted to
other languages.  Python and Boost/C++ are particularly well-equipped for good
GLM implementations in our opinion.

<h2>Disclaimer</h2>

This is <i>not</i> a formal introduction to GLMs.  We focus on illuminating
just a few key ideas, mostly from a numerical point of view and often focusing
on performance and scalability. We refer the reader to the references [3,5,7,9,12]
for complete introductions to linear models.

<p>

These are working notes. We'll continue to tweak and revise them.

<h2>Linear models</h2>

We begin by defining what we mean when we say "linear model" before moving on
to GLMs. Our presentation and notation generally follow Bj&ouml;rk[2].
Let $b\in\mathbb{R}^m, b\ne 0$ be a vector of measurements (called a response
vector) and let $A\in\mathbb{R}^{m\times n}$, $\mathrm{rank}(A)=n$, be a matrix
formed from data observations (called the model matrix).  Then
\[
Ax = b + e
\]
is what we mean when we say "linear model,"
where $x\in\mathbb{R}^n$ is a vector of model coefficients and the entries of
the residual error term $e\in\mathbb{R}^m$ are independent and
identically distributed random variables.  Given a vector $x$ of model
coefficients, we call the product $Ax$ a predictor of the response vector $b$.
We're leaving important definitions of "random variable" and "independent and
identically distributed" to the references.

<p><br/>

Given a model matrix $A$ and response vector $b$, there are many ways we might
go about solving for a vector of model coefficients $x$. We describe a
particularly natural one next.


<h2>Least squares</h2>

One idea we
might think of for solving for the model coefficients would be to compute
the ordinary least squares solution $x$ that minimizes
\[
\min_x ||Ax - b||^2,
\]
where $||\cdot||$ indicates Euclidean norm. Note that the conditions that
$\mathrm{rank}(A)=n$ and $b\ne 0$ imply that there exists a unique least
squares solution.  It turns out that the least squares
solution of linear models has important statistical properties shown in the
seminal work of Gauss[4] and later rediscovered by Markoff[6]. (The least
squares solution defines a <i>minimum variance unbiased estimator</i>, the
technical details of which we leave to the references, in particular see [5,7].)

<h2>Generalized linear models</h2>

Relaxing conditions on linear models by allowing the entries of $e$ to be
correlated or to be distributed with differing variances gives us generalized
linear models. We need a few technical definitions to make this precise.

<p>

Let $\epsilon$ be
a random variable with distribution function $F(\epsilon)$. The expected value $\mu$ and
the variance $\sigma^2$ of $\epsilon$ are defined as:
\[
E(\epsilon) = \mu = \int_{-\infty}^{\infty} \epsilon dF(\epsilon),
\qquad \sigma^2 = \int_{-\infty}^{\infty}(\epsilon-\mu)^2dF(\epsilon).
\]
When $e$ is a vector of random variables, we write $E(e)$ to mean the expected
value of each entry, and similarly we define the expected value of a matrix of
random variables to be the expected value of each entry.  Define the
variance-covariance matrix of a vector of random variables $e$ with mean vector
$\mu$ to be
\[
V(e) = E(ee^T) - \mu \mu^T.
\]

<p>

The assumption of independence and identical distribution among the random variables
in the error term $e$ of our earlier linear model required that $V(e) = \sigma^2 I$.
General Gauss-Markoff linear models relax this requirement. When we say "generalized
linear model" we mean a linear model of the form:
\[
Ax = b + e,\qquad V(e)=\sigma^2W^{-1},
\]
where $A$ is an $m\times n$ matrix of $\mathrm{rank}(A)=n$,
$e$ is a vector of random variables with zero mean and covariance matrix
$\sigma^2 W^{-1}$ and $W$ is a symmetric positive definite matrix.
See Bk&ouml;rk[2, pp. 160-165]
for a concise introduction to generalized linear models that we largely follow here.
The solution of the generalized linear model can be formulated as
\[
\min_x(Ax-b)^TW(Ax -b),
\]
with solution $x=(A^T W A)^{-1}A^T W b$.
The numerical solution of model problems of this form was carefully
analyzed by Paige[9].
We focus on simplified generalized models here
and assume that $W$ is a diagonal
matrix with positive entries along its diagonal.

<h2>Generalized linear models in practice</h2>

Generalized linear models typically
arise when we constrain the values that the predictor $Ax$ can take on and assume
a specific distribution function for the entries of $e$.
Constraints are important for many practical problems.  For example:
<ul>
<li> The response $b$ may represent binary 0/1 data and we want the predictor $Ax$
to at least be constrained to the interval $[0,1]$, in which case we might assume,
for example, a binomial distribution function for $e$.
<li> $b$ might represent counts and we desire the predictor to be nonnegative since
negative counts might not make any sense. In this case we might assume a
Poisson distribution function.
</ul>
A standard approach to solving the constrained problem is to transform it into
an unconstrained one by way of a <i>link function</i>. A link function is a
bijection chosen to map the constraint interval onto the real line (the
inverse link function maps the real line onto the constraint interval).
For example, the logistic function
\[
g(a) = \frac{1}{1 + \exp(-a)}
\]
maps arbitrary values $a\in\mathbb{R}$ into the interval $(0,1)$.

<p><br/>
Consider the binary 0/1 data example cited above.  Let
$A\in\mathbb{R}^{m\times n}$ be a given model matrix with $\mathrm{rank}(A)=n$, and let
$b\in\mathbb{R}^m$, $b\ne 0$, be a given response vector. The generalized linear model
problem is to compute an $x$ that satisfies:
\[
Ax = b + e,\qquad \mathrm{such\, that}\,\, Ax\in(0,1).
\]
We can formulate a related unconstrained problem
that solves for $x$ in
\[
g(Ax) = b + e,
\]
where $g$ is the logistic function applied elementwise to $Ax$. The problem
is no longer constrained because $g$ maps any value to the interval $(0,1)$.
But it's no longer linear either!

<p>
The choice of the nature of the error distribution defines the norm-weighting
matrix $W$ discussed in the previous section. If we assume that the entries of
$e$ consist of uncorrelated random variables with identical variance $1$, then
$W=I$ and the problem reduces to simple nonlinear least squares:
\[
\min_x ||g(Ax) - b||^2.
\]
That problem can be solved in R by several optimization methods,
including Broyden's quasi-Newton method as follows:
<pre>
# Input: A is an m by n model matrix, b a response vector of length m,
#        g is the inverse link function (g=binomial()$linkinv  for logistic).
# Output: The model coefficients x.

nnls_glm = function(A,b,g)
{
  f = function(x) crossprod(b - g(A %*% x))[]
  optim(rep(0,ncol(A)), fn=f, method="BFGS")$par
}
</pre>

If the entries of $e$ consist of uncorrelated random variables but with
different variances, then $W\ne I$ but $W$ is still a diagonal matrix. This is
common to many generalized linear model problems and results in a weighted
nonlinear least squares problem
\[
\min_x(g(Ax)-b)^TW(g(Ax) -b),
\]
typically solved by the iteratively reweighted least square method described next.

<h3>Algorithm IRLS: Iteratively reweighted least squares estimation</h3>
<h4>Input</h4>
Model matrix $A\in\mathbb{R}^{m\times n}$,
response vector $b\in\mathbb{R}^m$,
inverse link function $g$ and its derivative $g'$,
the variance $\mathrm{var}$ as a function of the mean associated with the distribution function of $e$,
termination tolerance $\mathrm{tol}$, maximum number of iterations itmax.
Note that "$/$" indicates element-wise division and the inverse link function $g$ and
its derivative and $\mathrm{var}$ are applied element-wise to their vector arguments.
<p>
<ol style="list-style:none">
<li style="margin: 0 0 10px 0;"> Let $x_1=0$.
<li> For $j=1,2,\ldots,\mathrm{itmax}$, do:
<ol style="list-style:none;">
  <li style="margin: 0 0 10px 0;"> Let $\eta = Ax_j$.
  <li style="margin: 0 0 10px 0;"> Let $z = \eta + \frac{b - g(\eta)}{g'(\eta)},\qquad$
  <li style="margin: 0 0 10px 0;"> Let $W = \mathrm{diag}(g'(\eta)^2/\mathrm{var}(g(\eta)))$.
  <li style="margin: 0 0 10px 0;"> Let $x_{j+1} = x_j + (A^T W A)^{-1} A^T W z$.
  <li> Stop if $||x_{j+1} - x_j|| \lt \mathrm{tol}$.
</ol>
</ol>
<hr>
It's remarkable how rapidly this simple algorithm converges. Before we dive in to
details, let's look at one possible R implementation of this algorithm and
compare it to R's <tt>glm</tt> function. The minimalist GLM function below
(closer to R's <tt>glm.fit</tt>, really)
takes as input a model matrix $A$, a response vector $b$, an R distribution family
object, and optional maximum number of iterations and tolerance values.
<p>
<pre>
minimalist_glm =
function(A, b, family=binomial, maxit=25, tol=1e-08)
{
  x = rep(0,ncol(A))
  for(j in 1:maxit)
  {
    eta    = A %*% x
    g      = family()$linkinv(eta)
    gprime = family()$mu.eta(eta)
    z      = eta + (b - g) / gprime
    W      = as.vector(gprime^2 / family()$variance(g))
    ATWA   = crossprod(A,(W * A))
    ATWz   = t(crossprod(W*z,A))
    xold   = x
    x      = solve(ATWA, ATWz, tol=2*.Machine$double.eps)
    if(sqrt(crossprod(x-xold)) &lt; tol) break
  }
  list(coefficients=x,iterations=j)
}
</pre>
<p>
Let's compare our minimalist estimator to R's <tt>glm</tt> function.
We use a real-world data example from the "mlmRev" package, which you may
need to install with <tt>install.packages("mlmRev")</tt>. The example
is described in Doug Bates' beautifully concise notes on GLMs[3].
<pre>
data("Contraception",package="mlmRev")

# Model estimated with R's glm function, returning model matrix and response
# in $x and $y, respectively:
MLE = glm(formula = use ~ age + I(age^2) + urban + livch,
          family = binomial, x=TRUE, data=Contraception)

# Model estimated with our radically stripped-down minimalist implementation:
mini = minimalist_glm(MLE$x, MLE$y, family=binomial)

print(data.frame(MLE=coef(MLE), minimalist=coef(mini)))
<font color="#00a">
                     MLE   minimalist
(Intercept) -0.949952124 -0.949952124
age          0.004583726  0.004583726
I(age^2)    -0.004286455 -0.004286455
urbanY       0.768097459  0.768097459
livch1       0.783112821  0.783112821
livch2       0.854904050  0.854904050
livch3+      0.806025052  0.806025052
</font>
</pre>
The results agree to displayed accuracy. Our minimalist implementation
converged after five iterations in this example.

<h2>IRLS Notes</h2>

IRLS solves a generalized linear model in each iteration:
\[
(A^T W A)^{-1} A^T W z,
\]
which is the just the solution to the (weighted) least squares problem:
\[
\min||W^{1/2}(Ax - b)||^2,
\]
so long as the diagonal weight matrix $W$ consists of positive entries along
the diagonal.  We make this a requirement of IRLS. Looking at how the weight
matrix $W$ is defined above, we see that this means that the derivative of the
inverse link function $g'$ applied to $Ax_j$ must never vanish. Note that
in the logistic regression case $g'(a) = g(a)(1-g(a))$ is <i>never</i> zero.

<p>

Clearly, some other numerical difficulties can arise.  What if 
$g'(\eta) \approx 0$ at some point? Or similarly, what if
$\mathrm{var}(g(\eta))\approx 0$
or $W$ is such that $A^TWA$ becomes extremely ill-conditioned?  A more
subtle problem occurs when the maximum ratio of any two entries along the
diagonal of $W$ is a big number.  Such problems are called <i>stiff</i> and
bring their own set of numerical difficulties.

<p>

Bj&ouml;rk[2, (pp. 165&mdash;171)] presents an excellent survey of solution
approaches for the weighted least squares problem.  Direct solution of the
normal equations (as illustrated in our simple R implementation of the IRLS
algorithm above) is potentially numerically unstable and not generally advised.
The brief survey in Bj&ouml;rk is really fascinating reading. His cited references
show us that even standard Household QR decomposition can give poor accuracy
for stiff problems unless it's modified to include column <i>and</i> row
pivoting. It turns out that the simplest numerically stable implementation for
solving the linear system within the IRLS method is the Givens QR algorithm,
shown by Anda and Park[1].
 
<p>

In practice, with judicious monitoring of the entries of $W$ (in particular the
stiffness $\max_j(W_{j,j})/\min_j(W_{j,j})$) and the quantities $g'(\eta)$ and
$\mathrm{var}(g(\eta))$, we can choose among the most stable and convenient
solution methods and simply warn the user when the algotihm gets into trouble
numerically. This is indeed what R does in its own <tt>glm.fit</tt>
implementation, which uses a Household QR decomposition of $WA$ with column
pivoting, modified to reveal the rank of the matrix at each step.


<h3>Connection to ordinary least squares</h3>

Note that when the link function is the identity function and the error
distribution function of $e$ is Gaussian with mean zero and variance
one, then the IRLS algorithm becomes:

<ol style="list-style:none">
<li style="margin: 0 0 10px 0;"> $x_1=0$
<li style="margin: 0 0 10px 0;"> $\eta = Ax_1 = 0$
<li style="margin: 0 0 10px 0;"> $z = \eta + \frac{b - g(\eta)}{g'(\eta)} = 0 + \frac{b - 0}{1} = b$
<li style="margin: 0 0 10px 0;"> $W = \mathrm{diag}(g'(\eta)^2/\mathrm{var}(g(\eta))) = \mathrm{diag}(1^2 /1)= I$
<li style="margin: 0 0 10px 0;"> $x_{2} = 0 + (A^T A)^{-1} A^T b$,
</ol>

which is just the ordinary least squares solution.

<h2>IRLS Performance considerations</h2>

<h2>Solving large GLMs by breaking them up into a series of smaller problems</h2>

<h2>Distributed parallel computation of IRLS</h2>



</section>

<section>
<h2>References</h2>

<ol>
<li> Anda, A. and Park, H., Self-scaling fast rotations for stiff least squares problems, Lin. Alg. Appl., 234, 1996, pp. 137-162.
<li> Bj&ouml;rck, &Aring;.,  Numerical Methods for Least Squares Problems, SIAM, Philadelphia, 1996.
<li> Bates, D., <a href="http://www.stat.wisc.edu/courses/st849-bates/lectures/GLMH.pdf">http://www.stat.wisc.edu/courses/st849-bates/lectures/GLMH.pdf</a>.
<li> Gauss, C. F., Theoria combinationis observationum erroribus minimis obnoxiae, Pars prior, 1863 (1st written 1821).
<li> Hastie, T. J. and Pregibon, D., Generalized linear models,
     Chapter 6 of Statistical Models in S, eds J. M. Chambers and T.
     J. Hastie, Wadsworth &amp; Brooks/Cole, 1992.
<li> Markoff, A., Wahrscheinlichheitsrechnug,  Leipzig, 1912.
<li> McCullagh P. and Nelder, J. A., Generalized Linear Models, Chapman and Hall, London 1989.
<li> O'Leary, D., Robust regression computation using iteratively reweighted least squares, Siam J. Mat. Anal. Appl., Vol. 11 No. 3, 1990, pp. 466-480.
<li> Paige, C. C., Fast numerically stable computations for generalized least squares problems, Siam J. Num. Anal., 16, 1979, pp. 165-171.
<li> The R project <a href="http://www.r-project.org">http://www.r-project.org</a>.
<li> Revolution Analytics <a href="http://revolutionanalytics.com">http://revolutionanalytics.com</a>.
<li> Zelen, M., Linear estimation and relted topics, in Survey of Numerical Analysis, Todd. J. ed, McGraw-Hill, New York, 1962, pp. 429-441.
</ol>
</secion>
</div>

<script src="javascripts/scale.fix.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</body>
</html>
